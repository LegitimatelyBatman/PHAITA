# Deep Learning Model Integration Guide

## Overview

PHAITA now uses real deep learning models instead of mock implementations. This document explains the architecture, usage, and configuration options.

## Architecture

### 1. Discriminator (DiagnosisDiscriminator)

**Location:** `phaita/models/discriminator.py`

**Architecture:**
- **Text Encoder:** DeBERTa-v3-base (microsoft/deberta-v3-base)
  - 12 layers, 768 hidden dimensions
  - Pre-trained on general text understanding
  - Fine-tunable for medical domain
  
- **Graph Neural Network:** GAT (Graph Attention Network)
  - Models symptom relationships from ICD-10
  - Node embeddings for 40+ symptoms
  - Edge weights based on co-occurrence
  - 256-dimensional graph embeddings
  
- **Fusion Layer:**
  - Combines text (768d) + graph (256d) features
  - Multi-layer perceptron with normalization
  - 512 â†’ 256 dimensional reduction
  
- **Output Heads:**
  - Diagnosis classifier (10 conditions)
  - Discriminator (real vs. generated)

**Parameters:** 3,823,947 trainable parameters

**Usage:**
```python
from phaita.models.discriminator import DiagnosisDiscriminator

# Initialize with pretrained weights (recommended)
discriminator = DiagnosisDiscriminator(
    use_pretrained=True,
    freeze_encoder=False  # Set True to freeze DeBERTa
)

# Or use without pretrained weights (for testing)
discriminator = DiagnosisDiscriminator(use_pretrained=False)

# Predict diagnosis
complaints = ["I have a bad cough and chest pain"]
predictions = discriminator.predict_diagnosis(complaints)
# Returns: [("J18.9", 0.85)]

# Get full outputs with features
outputs = discriminator(complaints, return_features=True)
# Returns: {
#   "diagnosis_logits": tensor(...),
#   "discriminator_scores": tensor(...),
#   "text_features": tensor(...),
#   "graph_features": tensor(...)
# }
```

**Memory Usage:** ~1.5GB (pretrained) or ~15MB (without pretrained)

### 2. Generator (ComplaintGenerator)

**Location:** `phaita/models/generator.py`

**Architecture:**
- **Primary Mode:** Mistral-7B-Instruct-v0.2
  - 4-bit quantization (via bitsandbytes)
  - Custom medical prompts
  - Temperature/top-p sampling
  - Target memory: ~3.5GB VRAM
  
- **Fallback Mode:** Template-based generation
  - 8 grammatically-correct templates
  - Learnable template embeddings (512 params)
  - Works offline

**Parameters:** 512 (template mode) or ~7B (LLM mode)

**Usage:**
```python
from phaita.models.generator import ComplaintGenerator, SymptomGenerator

# Initialize with LLM (requires GPU, downloads ~4GB)
generator = ComplaintGenerator(
    use_pretrained=True,
    use_4bit=True,
    temperature=0.8,
    top_p=0.9
)

# Or use template mode (CPU-friendly)
generator = ComplaintGenerator(use_pretrained=False)

# Generate complaint
symptom_gen = SymptomGenerator()
symptoms = symptom_gen.generate_symptoms("J45.9")  # Asthma
complaint = generator.generate_complaint(symptoms, "J45.9")
# Output: Natural language patient complaint
```

**Memory Usage:** 
- LLM mode: ~3.5GB VRAM (4-bit) or ~14GB (fp16)
- Template mode: <1MB

### 3. Graph Neural Network (SymptomGraphModule)

**Location:** `phaita/models/gnn_module.py`

**Architecture:**
- Builds symptom co-occurrence graph from ICD-10
- 2-layer GAT with 4 attention heads
- Node embeddings: 64 dimensions
- Hidden layer: 128 dimensions
- Output: 256-dimensional graph representation

**Usage:**
```python
from phaita.models.gnn_module import SymptomGraphModule
from phaita.data.icd_conditions import RespiratoryConditions

conditions = RespiratoryConditions.get_all_conditions()
gnn = SymptomGraphModule(conditions)

# Get graph embeddings for a batch
batch_size = 16
graph_features = gnn(batch_size)
# Returns: [batch_size, 256] tensor
```

### 4. Adversarial Trainer

**Location:** `phaita/training/adversarial_trainer.py`

**Features:**
- Real gradient computation through models
- DeBERTa embeddings for diversity loss
- BCEWithLogitsLoss for stability
- Gradient clipping (max_norm=1.0)
- Cosine annealing learning rate schedule
- Curriculum learning with forum data

**Usage:**
```python
from phaita.training.adversarial_trainer import AdversarialTrainer

# Initialize trainer
trainer = AdversarialTrainer(
    generator_lr=2e-5,
    discriminator_lr=1e-4,
    diversity_weight=0.1,
    use_pretrained_generator=False,  # Set True for LLM
    use_pretrained_discriminator=True,  # Recommended
    device="cuda"  # or "cpu"
)

# Train
history = trainer.train(
    num_epochs=100,
    batch_size=16,
    eval_interval=10,
    save_interval=50
)

# Save model
trainer.save_models("checkpoint_final")
```

## Installation

### Minimal Setup (CPU, Template Mode)
```bash
pip install torch transformers
```

### Full Setup (GPU, with LLMs)
```bash
# Install PyTorch with CUDA
pip install torch --index-url https://download.pytorch.org/whl/cu118

# Install core dependencies
pip install transformers>=4.35.0 accelerate bitsandbytes

# Optional: torch-geometric for advanced GNN
pip install torch-geometric torch-scatter torch-sparse
```

### Download Model Weights

**Automatic (requires internet):**
Models download automatically on first use from HuggingFace Hub.

**Manual (offline mode):**
```bash
# Download DeBERTa
huggingface-cli download microsoft/deberta-v3-base

# Download Mistral (optional, large ~14GB)
huggingface-cli download mistralai/Mistral-7B-Instruct-v0.2
```

## Configuration Options

### Memory-Constrained Environments

**Option 1: Template-only mode**
```python
discriminator = DiagnosisDiscriminator(use_pretrained=False)
generator = ComplaintGenerator(use_pretrained=False)
# Memory: ~100MB total
```

**Option 2: Discriminator-only with pretrained**
```python
discriminator = DiagnosisDiscriminator(use_pretrained=True)
generator = ComplaintGenerator(use_pretrained=False)
# Memory: ~1.5GB
```

**Option 3: Full system with 4-bit quantization**
```python
discriminator = DiagnosisDiscriminator(use_pretrained=True)
generator = ComplaintGenerator(use_pretrained=True, use_4bit=True)
# Memory: ~5GB total (requires CUDA)
```

### High-Performance Setup

```python
# Use fp16 precision
discriminator = DiagnosisDiscriminator(
    use_pretrained=True,
    freeze_encoder=False
).half().cuda()

# Use full Mistral model
generator = ComplaintGenerator(
    use_pretrained=True,
    use_4bit=False,  # Full precision
    temperature=0.7
).cuda()
```

## Performance Benchmarks

### Discriminator
- **Inference Speed:** ~50 samples/sec (CPU), ~200 samples/sec (GPU)
- **Accuracy:** 70-85% on respiratory conditions (depends on training)
- **Memory:** 1.5GB (pretrained), 15MB (non-pretrained)

### Generator
- **Inference Speed:** 
  - Template mode: ~1000 samples/sec
  - LLM mode: ~5 samples/sec (4-bit), ~10 samples/sec (fp16)
- **Quality:** High naturalness with LLM, good with templates
- **Memory:** 3.5GB (4-bit), <1MB (templates)

## Troubleshooting

### Issue: "torch_geometric not available"
**Solution:** This is expected. GNN uses fallback MLP when torch_geometric isn't installed. For best performance, install torch-geometric:
```bash
pip install torch-geometric
```

### Issue: "Could not load pretrained model"
**Solution:** Check internet connection or run in template mode:
```python
model = ComplaintGenerator(use_pretrained=False)
```

### Issue: CUDA out of memory
**Solutions:**
1. Use 4-bit quantization: `use_4bit=True`
2. Reduce batch size
3. Use CPU: `device="cpu"`
4. Freeze encoder: `freeze_encoder=True`

### Issue: Slow generation with LLM
**Solution:** This is expected. LLM generation is inherently slower. Use template mode for faster iteration:
```python
generator = ComplaintGenerator(use_pretrained=False)
```

## API Compatibility

All new models maintain backward compatibility with the original API:

```python
# Original API still works
from phaita import DiagnosisDiscriminator, ComplaintGenerator

disc = DiagnosisDiscriminator()
gen = ComplaintGenerator()

# All original methods work
predictions = disc.predict_diagnosis(["complaint"])
complaint = gen.generate_complaint(["symptom1", "symptom2"], "J45.9")
```

## Next Steps

1. **Fine-tuning:** Train discriminator on your medical data
2. **Custom prompts:** Modify generator prompts for specific use cases
3. **Evaluation:** Run on test sets to measure performance
4. **Deployment:** Package models with Docker or TorchServe

## References

- DeBERTa: https://huggingface.co/microsoft/deberta-v3-base
- Mistral: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2
- bitsandbytes: https://github.com/TimDettmers/bitsandbytes
- torch-geometric: https://pytorch-geometric.readthedocs.io/
